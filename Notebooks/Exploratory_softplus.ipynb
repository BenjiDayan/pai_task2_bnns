{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('../')\n",
    "from solution_debugging import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = os.curdir\n",
    "output_dir = os.curdir\n",
    "raw_train_data = np.load(os.path.join(data_dir, 'train_data.npz'))\n",
    "x_train = torch.from_numpy(raw_train_data['train_x']).reshape([-1, 784])\n",
    "y_train = torch.from_numpy(raw_train_data['train_y']).long()\n",
    "dataset_train = torch.utils.data.TensorDataset(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using a BayesNet model\n",
      "initialising bayesian layer of size 784 x 100\n",
      "initialising bayesian layer of size 100 x 100\n",
      "initialising bayesian layer of size 100 x 10\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch.autograd.anomaly_mode.set_detect_anomaly at 0x17d09db8390>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_mini = x_train[:2000]\n",
    "y_train_mini = y_train[:2000]\n",
    "dataset_train_mini = torch.utils.data.TensorDataset(x_train, y_train)\n",
    "model = Model(use_densenet=False, num_epochs=1)\n",
    "torch.autograd.set_detect_anomaly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.train(dataset_train_mini)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BayesianLayer(\n",
      "  (weights_prior): MultivariateDiagonalGaussian()\n",
      "  (bias_prior): MultivariateDiagonalGaussian()\n",
      "  (weights_var_posterior): MultivariateDiagonalGaussian()\n",
      "  (bias_var_posterior): MultivariateDiagonalGaussian()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "bl = model.network.layers[0]\n",
    "print(bl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initialising bayesian layer of size 784 x 100\n",
      "initialising bayesian layer of size 100 x 100\n",
      "initialising bayesian layer of size 100 x 10\n"
     ]
    }
   ],
   "source": [
    "in_features = 28*28\n",
    "out_features = 10\n",
    "hidden_features = (100,100)\n",
    "feature_sizes = (in_features,) + hidden_features + (out_features,)\n",
    "num_affine_maps = len(feature_sizes) - 1\n",
    "\n",
    "\n",
    "\"\"\"We recreate the network but using softplus=False - for some reason this doesn't lead us to the second time gradient\n",
    "calculation errors (saying that )\n",
    "\n",
    "UserWarning: Error detected in SoftplusBackward. Traceback of forward call that caused the error:\n",
    "  File \"C:/Users/benja/Google Drive/eth_courses/pai/task2_bnn/solution_debugging.py\", line 629, in <module>\n",
    "  \n",
    "  ...\n",
    "  \n",
    "      self.bias_var_posterior = MultivariateDiagonalGaussian(torch.nn.Parameter(torch.zeros((out_features,))),torch.nn.Parameter(torch.ones((out_features,))), softplus=softplus)\n",
    "  File \"C:/Users/benja/Google Drive/eth_courses/pai/task2_bnn/solution_debugging.py\", line 407, in __init__\n",
    "    normal = torch.distributions.Normal(self.mu, F.softplus(self.rho))\n",
    " (Triggered internally at  ..\\torch\\csrc\\autograd\\python_anomaly_mode.cpp:104.)\n",
    "  allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
    "Traceback (most recent call last):\n",
    "\n",
    "...\n",
    "\n",
    "File \"C:\\Users\\benja\\Anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py\", line 149, in backward\n",
    "    allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
    "RuntimeError: Trying to backward through the graph a second time (or directly access saved variables after they have\n",
    "already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad().\n",
    "Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved\n",
    "variables after calling backward.\n",
    "\n",
    "\n",
    "NBNB!!!\n",
    "\n",
    "retain_graph=True doesn't actually fix things, gives different error:\n",
    "RuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation:\n",
    "[torch.FloatTensor [10]] is at version 1; expected version 0 instead. Hint: the backtrace further above shows\n",
    "the operation that failed to compute its gradient. The variable in question was changed in there or anywhere later. Good luck!\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "model.network.layers = nn.ModuleList([\n",
    "            BayesianLayer(feature_sizes[idx], feature_sizes[idx + 1], bias=True, softplus=False)\n",
    "            for idx in range(num_affine_maps)\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BayesNet(\n",
       "  (layers): ModuleList(\n",
       "    (0): BayesianLayer(\n",
       "      (weights_prior): MultivariateDiagonalGaussian()\n",
       "      (bias_prior): MultivariateDiagonalGaussian()\n",
       "      (weights_var_posterior): MultivariateDiagonalGaussian()\n",
       "      (bias_var_posterior): MultivariateDiagonalGaussian()\n",
       "    )\n",
       "    (1): BayesianLayer(\n",
       "      (weights_prior): MultivariateDiagonalGaussian()\n",
       "      (bias_prior): MultivariateDiagonalGaussian()\n",
       "      (weights_var_posterior): MultivariateDiagonalGaussian()\n",
       "      (bias_var_posterior): MultivariateDiagonalGaussian()\n",
       "    )\n",
       "    (2): BayesianLayer(\n",
       "      (weights_prior): MultivariateDiagonalGaussian()\n",
       "      (bias_prior): MultivariateDiagonalGaussian()\n",
       "      (weights_var_posterior): MultivariateDiagonalGaussian()\n",
       "      (bias_var_posterior): MultivariateDiagonalGaussian()\n",
       "    )\n",
       "  )\n",
       "  (activation): ReLU()\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "batch_x = x_train[:3]\n",
    "batch_y = y_train[:3]\n",
    "\n",
    "# out, log_prior, log_var_ptior = bl.forward(x_batch)\n",
    "optimizer = torch.optim.Adam(model.network.parameters(), lr=1e-3)\n",
    "model.network.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.network.zero_grad()\n",
    "\n",
    "out, log_prior, log_var_ptior = model.network.forward(batch_x)\n",
    "f = out, log_prior, log_var_ptior\n",
    "\n",
    "class_predicted_probs = F.softmax(f[0], dim=1)\n",
    "data_predicted_probs = torch.sum(class_predicted_probs * F.one_hot(batch_y, num_classes=10), dim=1)\n",
    "eps=1e-5\n",
    "data_predicted_probs = torch.max(data_predicted_probs, torch.tensor(eps).expand_as(data_predicted_probs))\n",
    "num_not_maxed = torch.sum(data_predicted_probs > eps)\n",
    "log_prob_of_data_given_network = torch.sum(\n",
    "    torch.log( data_predicted_probs)\n",
    ")\n",
    "# loss = pi_i * (torch.sum(f[2]) - torch.sum(f[1])) - log_prob_of_data_given_network\n",
    "loss = (torch.sum(f[2]) - torch.sum(f[1])) - log_prob_of_data_given_network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss.backward(retain_graph=True)\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([1.0010, 0.9990, 0.9990, 1.0010, 1.0010, 1.0010, 0.9990, 1.0010, 1.0010,\n",
       "        1.0010, 1.0010, 0.9990, 1.0010, 0.9990, 1.0010, 0.9990, 1.0010, 0.9990,\n",
       "        0.9990, 1.0010, 1.0010, 1.0010, 0.9990, 0.9990, 0.9990, 1.0010, 0.9990,\n",
       "        1.0010, 1.0010, 1.0010, 1.0010, 1.0010, 1.0010, 1.0010, 1.0010, 1.0010,\n",
       "        1.0010, 0.9990, 1.0010, 0.9990, 0.9990, 1.0010, 1.0010, 0.9990, 0.9990,\n",
       "        1.0010, 0.9990, 1.0010, 1.0010, 0.9990, 1.0010, 1.0010, 1.0010, 1.0010,\n",
       "        1.0010, 0.9990, 0.9990, 0.9990, 0.9990, 1.0010, 0.9990, 0.9990, 1.0010,\n",
       "        1.0010, 1.0010, 0.9990, 1.0010, 0.9990, 0.9990, 1.0010, 1.0010, 1.0010,\n",
       "        0.9990, 1.0010, 0.9990, 0.9990, 1.0010, 0.9990, 1.0010, 0.9990, 1.0010,\n",
       "        0.9990, 0.9990, 1.0010, 0.9990, 1.0010, 0.9990, 1.0010, 1.0010, 1.0010,\n",
       "        1.0010, 1.0010, 0.9990, 1.0010, 1.0010, 1.0010, 0.9990, 1.0010, 1.0010,\n",
       "        1.0010, 1.0010, 1.0010, 1.0010, 1.0010, 1.0010, 1.0010, 1.0010, 1.0010,\n",
       "        0.9990, 1.0010, 0.9990, 1.0010, 1.0010, 1.0010, 1.0010, 0.9990, 1.0010,\n",
       "        1.0010, 1.0010, 0.9990, 0.9990, 0.9990, 1.0010, 1.0010, 1.0010, 0.9990,\n",
       "        0.9990, 1.0010, 1.0010, 0.9990, 1.0010, 0.9990, 1.0010, 1.0010, 1.0010,\n",
       "        1.0010, 1.0010, 1.0010, 1.0010, 1.0010, 1.0010, 0.9990, 1.0010, 1.0010,\n",
       "        0.9990, 1.0010, 1.0010, 1.0010, 1.0010, 1.0010, 0.9990, 0.9990, 1.0010,\n",
       "        0.9990, 1.0010, 1.0010, 1.0010, 1.0010, 1.0010, 1.0010, 1.0010, 1.0010,\n",
       "        1.0010, 0.9990, 0.9990, 1.0010, 0.9990, 1.0010, 0.9990, 1.0010, 0.9990,\n",
       "        1.0010, 0.9990, 1.0010, 1.0010, 1.0010, 1.0010, 1.0010, 0.9990, 0.9990,\n",
       "        1.0010, 0.9990, 1.0010, 0.9990, 1.0010, 1.0010, 1.0010, 1.0010, 1.0010,\n",
       "        0.9990, 1.0010, 1.0010, 1.0010, 1.0010, 0.9990, 0.9990, 1.0010, 1.0010,\n",
       "        1.0010, 1.0010, 1.0010, 1.0010, 1.0010, 1.0010, 1.0010, 1.0010, 0.9990,\n",
       "        1.0010, 1.0010, 0.9990, 1.0010, 1.0010, 1.0010, 0.9990, 0.9990, 1.0010,\n",
       "        1.0010, 1.0010, 1.0010, 0.9990, 1.0010, 1.0010, 1.0010, 1.0010, 1.0010,\n",
       "        1.0010, 1.0010, 0.9990, 1.0010, 1.0010, 1.0010, 1.0010, 1.0010, 0.9990,\n",
       "        0.9990, 1.0010, 0.9990, 1.0010, 0.9990, 0.9990, 1.0010, 0.9990, 0.9990,\n",
       "        1.0010, 1.0010, 1.0010, 1.0010, 0.9990, 1.0010, 1.0010, 1.0010, 1.0010,\n",
       "        1.0010, 0.9990, 1.0010, 1.0010, 1.0010, 1.0010, 1.0010, 1.0010, 0.9990,\n",
       "        1.0010, 0.9990, 0.9990, 0.9990, 1.0010, 1.0010, 0.9990, 1.0010, 1.0010,\n",
       "        1.0010, 1.0010, 1.0010, 1.0010, 1.0010, 0.9990, 0.9990, 1.0010, 0.9990,\n",
       "        1.0010, 1.0010, 1.0010, 0.9990, 1.0010, 1.0010, 1.0010, 1.0010, 0.9990,\n",
       "        1.0010, 0.9990, 1.0010, 1.0010, 1.0010, 1.0010, 0.9990, 0.9990, 1.0010,\n",
       "        0.9990, 0.9990, 1.0010, 1.0010, 1.0010, 1.0010, 0.9990, 0.9990, 1.0010,\n",
       "        1.0010, 1.0010, 1.0010, 0.9990, 1.0010, 1.0010, 1.0010, 0.9990, 1.0010,\n",
       "        0.9990, 1.0010, 1.0010, 0.9990, 1.0010, 1.0010, 1.0010, 1.0010, 1.0010,\n",
       "        0.9990, 1.0010, 1.0010, 0.9990, 1.0010, 1.0010, 1.0010, 1.0010, 1.0010,\n",
       "        0.9990, 0.9990, 0.9990, 0.9990, 1.0010, 1.0010, 1.0010, 0.9990, 1.0010,\n",
       "        1.0010, 1.0010, 1.0010, 0.9990, 1.0010, 0.9990, 1.0010, 1.0010, 1.0010,\n",
       "        0.9990, 1.0010, 1.0010, 1.0010, 1.0010, 0.9990, 1.0010, 0.9990, 0.9990,\n",
       "        1.0010, 0.9990, 0.9990, 1.0010, 1.0010, 1.0010, 1.0010, 0.9990, 1.0010,\n",
       "        1.0010, 1.0010, 1.0010, 0.9990, 0.9990, 0.9990, 1.0010, 1.0010, 0.9990,\n",
       "        0.9990, 0.9990, 0.9990, 1.0010, 1.0010, 1.0010, 0.9990, 1.0010, 1.0010,\n",
       "        0.9990, 1.0010, 1.0010, 1.0010, 0.9990, 1.0010, 1.0010, 1.0010, 1.0010,\n",
       "        0.9990, 1.0010, 1.0010, 1.0010, 0.9990, 0.9990, 1.0010, 0.9990, 1.0010,\n",
       "        1.0010, 1.0010, 0.9990, 0.9990, 1.0010, 1.0010, 0.9990, 0.9990, 1.0010,\n",
       "        1.0010, 1.0010, 0.9990, 0.9990, 0.9990, 0.9990, 1.0010, 1.0010, 1.0010,\n",
       "        0.9990, 1.0010, 1.0010, 1.0010, 1.0010, 1.0010, 1.0010, 1.0010, 0.9990,\n",
       "        0.9990, 1.0010, 0.9990, 1.0010, 1.0010, 0.9990, 1.0010, 1.0010, 1.0010,\n",
       "        0.9990, 1.0010, 1.0010, 0.9990, 0.9990, 1.0010, 1.0010, 0.9990, 1.0010,\n",
       "        0.9990, 1.0010, 1.0010, 1.0010, 0.9990, 1.0010, 1.0010, 1.0010, 1.0010,\n",
       "        1.0010, 1.0010, 1.0010, 1.0010, 1.0010, 0.9990, 1.0010, 1.0010, 1.0010,\n",
       "        0.9990, 1.0010, 1.0010, 0.9990, 0.9990, 1.0010, 1.0010, 0.9990, 1.0010,\n",
       "        1.0010, 1.0010, 0.9990, 1.0010, 0.9990, 0.9990, 0.9990, 1.0010, 1.0010,\n",
       "        0.9990, 1.0010, 1.0010, 0.9990, 0.9990, 1.0010, 1.0010, 1.0010, 1.0010,\n",
       "        1.0010, 1.0010, 1.0010, 1.0010, 1.0010, 0.9990, 1.0010, 1.0010, 0.9990,\n",
       "        1.0010, 0.9990, 0.9990, 0.9990, 1.0010, 1.0010, 0.9990, 1.0010, 0.9990,\n",
       "        1.0010, 0.9990, 1.0010, 1.0010, 1.0010, 0.9990, 1.0010, 1.0010, 0.9990,\n",
       "        0.9990, 1.0010, 1.0010, 1.0010, 1.0010, 1.0010, 1.0010, 1.0010, 0.9990,\n",
       "        1.0010, 1.0010, 1.0010, 0.9990, 1.0010, 1.0010, 1.0010, 1.0010, 1.0010,\n",
       "        0.9990, 1.0010, 0.9990, 1.0010, 1.0010, 1.0010, 1.0010, 1.0010, 0.9990,\n",
       "        0.9990, 1.0010, 1.0010, 1.0010, 1.0010, 1.0010, 1.0010, 1.0010, 0.9990,\n",
       "        1.0010, 0.9990, 1.0010, 1.0010, 1.0010, 1.0010, 1.0010, 0.9990, 1.0010,\n",
       "        0.9990, 1.0010, 0.9990, 0.9990, 1.0010, 0.9990, 0.9990, 0.9990, 0.9990,\n",
       "        0.9990, 0.9990, 1.0010, 1.0010, 1.0010, 0.9990, 1.0010, 0.9990, 1.0010,\n",
       "        1.0010, 0.9990, 0.9990, 1.0010, 1.0010, 0.9990, 0.9990, 1.0010, 0.9990,\n",
       "        1.0010, 1.0010, 0.9990, 0.9990, 0.9990, 1.0010, 0.9990, 1.0010, 1.0010,\n",
       "        0.9990, 1.0010, 0.9990, 1.0010, 0.9990, 0.9990, 0.9990, 1.0010, 1.0010,\n",
       "        0.9990, 1.0010, 1.0010, 1.0010, 0.9990, 0.9990, 1.0010, 0.9990, 1.0010,\n",
       "        0.9990, 1.0010, 1.0010, 1.0010, 0.9990, 1.0010, 1.0010, 1.0010, 0.9990,\n",
       "        0.9990, 1.0010, 0.9990, 1.0010, 1.0010, 1.0010, 0.9990, 1.0010, 0.9990,\n",
       "        1.0010, 1.0010, 1.0010, 1.0010, 0.9990, 0.9990, 1.0010, 1.0010, 1.0010,\n",
       "        1.0010, 0.9990, 1.0010, 1.0010, 1.0010, 0.9990, 0.9990, 1.0010, 0.9990,\n",
       "        1.0010, 0.9990, 1.0010, 0.9990, 1.0010, 1.0010, 0.9990, 1.0010, 1.0010,\n",
       "        1.0010, 1.0010, 0.9990, 0.9990, 0.9990, 0.9990, 1.0010, 1.0010, 0.9990,\n",
       "        1.0010, 1.0010, 1.0010, 1.0010, 1.0010, 1.0010, 1.0010, 1.0010, 1.0010,\n",
       "        0.9990, 1.0010, 0.9990, 1.0010, 1.0010, 0.9990, 0.9990, 1.0010, 0.9990,\n",
       "        1.0010, 0.9990, 1.0010, 1.0010, 0.9990, 1.0010, 1.0010, 0.9990, 1.0010,\n",
       "        0.9990, 1.0010, 1.0010, 1.0010, 0.9990, 1.0010, 1.0010, 0.9990, 1.0010,\n",
       "        1.0010, 0.9990, 1.0010, 0.9990, 0.9990, 1.0010, 1.0010, 1.0010, 1.0010,\n",
       "        0.9990, 1.0010, 1.0010, 0.9990, 1.0010, 1.0010, 1.0010, 0.9990, 1.0010,\n",
       "        0.9990, 0.9990, 1.0010, 0.9990, 1.0010, 1.0010, 0.9990, 1.0010, 1.0010,\n",
       "        1.0010, 0.9990, 1.0010, 1.0010, 1.0010, 1.0010, 1.0010, 1.0010, 1.0010,\n",
       "        0.9990, 1.0010, 0.9990, 1.0010, 1.0010, 1.0010, 1.0010, 1.0010, 1.0010,\n",
       "        0.9990, 0.9990, 0.9990, 1.0010, 1.0010, 1.0010, 1.0010, 1.0010, 0.9990,\n",
       "        1.0010, 0.9990, 0.9990, 1.0010, 0.9990, 1.0010, 1.0010, 0.9990, 1.0010,\n",
       "        1.0010, 1.0010, 1.0010, 1.0010, 0.9990, 1.0010, 1.0010, 0.9990, 1.0010,\n",
       "        0.9990, 0.9990, 1.0010, 1.0010, 0.9990, 1.0010, 1.0010, 1.0010, 0.9990,\n",
       "        0.9990, 0.9990, 1.0010, 1.0010, 1.0010, 1.0010, 1.0010, 1.0010, 1.0010,\n",
       "        1.0010, 1.0010, 1.0010, 1.0010, 0.9990, 1.0010, 1.0010, 1.0010, 1.0010,\n",
       "        0.9990, 1.0010, 1.0010, 0.9990, 0.9990, 0.9990, 0.9990, 1.0010, 1.0010,\n",
       "        1.0010, 1.0010, 1.0010, 1.0010, 1.0010, 1.0010, 1.0010, 1.0010, 1.0010,\n",
       "        1.0010, 1.0010, 0.9990, 1.0010, 0.9990, 1.0010, 1.0010, 0.9990, 0.9990,\n",
       "        0.9990, 1.0010, 0.9990, 1.0010, 0.9990, 0.9990, 1.0010, 1.0010, 1.0010,\n",
       "        0.9990, 1.0010, 0.9990, 0.9990, 0.9990, 1.0010, 1.0010, 1.0010, 0.9990,\n",
       "        1.0010, 1.0010, 0.9990, 1.0010, 0.9990, 0.9990, 0.9990, 1.0010, 0.9990,\n",
       "        1.0010, 0.9990, 1.0010, 0.9990, 1.0010, 0.9990, 1.0010, 1.0010, 1.0010,\n",
       "        1.0010, 1.0010, 1.0010, 1.0010, 1.0010, 1.0010, 1.0010, 1.0010, 1.0010,\n",
       "        1.0010, 1.0010, 0.9990, 1.0010, 1.0010, 1.0010, 0.9990, 1.0010, 0.9990,\n",
       "        1.0010, 0.9990, 0.9990, 1.0010, 1.0010, 0.9990, 1.0010, 1.0010, 0.9990,\n",
       "        0.9990, 1.0010, 0.9990, 1.0010, 0.9990, 1.0010, 1.0010, 1.0010, 0.9990,\n",
       "        0.9990, 1.0010, 1.0010, 0.9990, 0.9990, 1.0010, 1.0010, 0.9990, 0.9990,\n",
       "        1.0010, 1.0010, 1.0010, 0.9990, 0.9990, 0.9990, 1.0010, 0.9990, 0.9990,\n",
       "        0.9990, 1.0010, 1.0010, 1.0010, 1.0010, 1.0010, 1.0010, 0.9990, 1.0010,\n",
       "        1.0010, 0.9990, 0.9990, 1.0010, 1.0010, 1.0010, 1.0010, 1.0010, 0.9990,\n",
       "        1.0010, 1.0010, 0.9990, 1.0010, 1.0010, 0.9990, 0.9990, 1.0010, 0.9990,\n",
       "        0.9990, 1.0010, 0.9990, 1.0010, 1.0010, 1.0010, 1.0010, 1.0010, 1.0010,\n",
       "        0.9990, 1.0010, 0.9990, 1.0010, 1.0010, 1.0010, 1.0010, 1.0010, 1.0010,\n",
       "        0.9990, 0.9990, 1.0010, 1.0010, 1.0010, 1.0010, 1.0010, 1.0010, 1.0010,\n",
       "        1.0010, 0.9990, 1.0010, 0.9990, 1.0010, 0.9990, 1.0010, 1.0010, 0.9990,\n",
       "        0.9990, 0.9990, 1.0010, 1.0010, 1.0010, 1.0010, 1.0010, 1.0010, 1.0010,\n",
       "        1.0010], requires_grad=True)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.network.layers[-1].weights_var_posterior.rho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "batch_x = x_train[3:6]\n",
    "batch_y = y_train[3:6]\n",
    "\n",
    "\n",
    "model.network.zero_grad()\n",
    "\n",
    "out, log_prior, log_var_ptior = model.network.forward(batch_x)\n",
    "f = out, log_prior, log_var_ptior\n",
    "\n",
    "class_predicted_probs = F.softmax(f[0], dim=1)\n",
    "data_predicted_probs = torch.sum(class_predicted_probs * F.one_hot(batch_y, num_classes=10), dim=1)\n",
    "eps=1e-5\n",
    "data_predicted_probs = torch.max(data_predicted_probs, torch.tensor(eps).expand_as(data_predicted_probs))\n",
    "num_not_maxed = torch.sum(data_predicted_probs > eps)\n",
    "log_prob_of_data_given_network = torch.sum(\n",
    "    torch.log( data_predicted_probs)\n",
    ")\n",
    "# loss = pi_i * (torch.sum(f[2]) - torch.sum(f[1])) - log_prob_of_data_given_network\n",
    "loss = (torch.sum(f[2]) - torch.sum(f[1])) - log_prob_of_data_given_network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.5452,  0.7160, -0.5759,  ..., -1.4657,  0.2981,  0.6082])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bl.weights_var_posterior.mu.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([0., 0., 0.,  ..., 0., 0., 0.], requires_grad=True)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.network.layers[0].weights_var_posterior.mu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
